{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq # Used to read the data\n",
    "import os \n",
    "import numpy as np\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split \n",
    "from keras import backend as K \n",
    "from keras import optimizers\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from keras.callbacks import *\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # select how many folds will be created\n",
    "# N_SPLITS = 5\n",
    "# # it is just a constant with the measurements data size\n",
    "# sample_size = 800000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>signal_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_measurement</th>\n",
       "      <th>phase</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      signal_id  target\n",
       "id_measurement phase                   \n",
       "0              0              0       0\n",
       "               1              1       0\n",
       "               2              2       0\n",
       "1              0              3       1\n",
       "               1              4       1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the training set metadata, defines which signals are in which order in the data\n",
    "train_meta = pd.read_csv('../vsb-power-line-fault-detection/metadata_train.csv')\n",
    "# set index, it makes the data access much faster\n",
    "train_meta = train_meta.set_index(['id_measurement', 'phase'])\n",
    "train_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>signal_id</th>\n",
       "      <th>id_measurement</th>\n",
       "      <th>phase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8712</td>\n",
       "      <td>2904</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8713</td>\n",
       "      <td>2904</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8714</td>\n",
       "      <td>2904</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8715</td>\n",
       "      <td>2905</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8716</td>\n",
       "      <td>2905</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   signal_id  id_measurement  phase\n",
       "0       8712            2904      0\n",
       "1       8713            2904      1\n",
       "2       8714            2904      2\n",
       "3       8715            2905      0\n",
       "4       8716            2905      1"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the test set metadata, defines which signals are in which order in the data\n",
    "test_meta = pd.read_csv('../vsb-power-line-fault-detection/metadata_test.csv')\n",
    "# set index, it makes the data access much faster\n",
    "#test_meta = test_meta.set_index(['id_measurement', 'phase'])\n",
    "test_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1393920, 20)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the training dataset\n",
    "df_train_pre = pd.read_csv(\"../code/my_train.csv.gz\", compression=\"gzip\")\n",
    "df_train_pre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3253920, 20)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the test dataset\n",
    "df_test_pre = pd.read_csv(\"../code/my_test.csv.gz\", compression=\"gzip\")\n",
    "df_test_pre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop the extra column in each of the datasets above\n",
    "df_train_pre.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "df_test_pre.drop(\"Unnamed: 0\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8712, 160, 19)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the shape of the dataset into a 3D format for the LSTM\n",
    "# The sequence size is 160\n",
    "X = df_train_pre.values.reshape(8712, 160, 19)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20337, 160, 19)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the shape of the dataset into a 3D format for the LSTM\n",
    "# The sequence size is 160\n",
    "X_test = df_test_pre.values.reshape(20337, 160, 19)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8712,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the labels for the training dataset\n",
    "y = train_meta[\"target\"]\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_preds = np.zeros(X.shape[0])\n",
    "label_predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the one-way LSTM model\n",
    "def create_model(input_data):\n",
    "    input_shape = input_data.shape\n",
    "    inp = Input(shape=(input_shape[1], input_shape[2],), name=\"input_signal\")\n",
    "    x = LSTM(128, return_sequences=True, name=\"lstm1\")(inp)\n",
    "    x = LSTM(64, return_sequences=False, name=\"lstm2\")(x)   \n",
    "    x = Dense(128, activation=\"relu\", name=\"dense1\")(x)\n",
    "    x = Dense(64, activation=\"relu\", name=\"dense2\")(x)\n",
    "    x = Dense(1, activation='sigmoid', name=\"output\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam',metrics = ['mae', 'acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning fold 1\n",
      "Train on 4355 samples, validate on 4357 samples\n",
      "Epoch 1/50\n",
      "4355/4355 [==============================] - 17s 4ms/step - loss: 0.2760 - mean_absolute_error: 0.1627 - acc: 0.9160 - val_loss: 0.1657 - val_mean_absolute_error: 0.0952 - val_acc: 0.9396\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.16567, saving model to best_model.h5\n",
      "Epoch 2/50\n",
      "4355/4355 [==============================] - 13s 3ms/step - loss: 0.1465 - mean_absolute_error: 0.0794 - acc: 0.9449 - val_loss: 0.1533 - val_mean_absolute_error: 0.0900 - val_acc: 0.9456\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.16567 to 0.15331, saving model to best_model.h5\n",
      "Epoch 3/50\n",
      "4355/4355 [==============================] - 14s 3ms/step - loss: 0.1339 - mean_absolute_error: 0.0789 - acc: 0.9502 - val_loss: 0.1577 - val_mean_absolute_error: 0.0645 - val_acc: 0.9449\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.15331\n",
      "Epoch 4/50\n",
      "4355/4355 [==============================] - 14s 3ms/step - loss: 0.1325 - mean_absolute_error: 0.0756 - acc: 0.9490 - val_loss: 0.1539 - val_mean_absolute_error: 0.0660 - val_acc: 0.9470\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.15331\n",
      "Epoch 5/50\n",
      "4355/4355 [==============================] - 13s 3ms/step - loss: 0.1275 - mean_absolute_error: 0.0699 - acc: 0.9509 - val_loss: 0.1380 - val_mean_absolute_error: 0.0855 - val_acc: 0.9479\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.15331 to 0.13796, saving model to best_model.h5\n",
      "Epoch 6/50\n",
      "4355/4355 [==============================] - 14s 3ms/step - loss: 0.1299 - mean_absolute_error: 0.0778 - acc: 0.9401 - val_loss: 0.1367 - val_mean_absolute_error: 0.0877 - val_acc: 0.9440\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.13796 to 0.13667, saving model to best_model.h5\n",
      "Epoch 7/50\n",
      "4355/4355 [==============================] - 14s 3ms/step - loss: 0.1234 - mean_absolute_error: 0.0704 - acc: 0.9529 - val_loss: 0.1362 - val_mean_absolute_error: 0.0805 - val_acc: 0.9490\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.13667 to 0.13619, saving model to best_model.h5\n",
      "Epoch 8/50\n",
      "4355/4355 [==============================] - 14s 3ms/step - loss: 0.1197 - mean_absolute_error: 0.0699 - acc: 0.9522 - val_loss: 0.1414 - val_mean_absolute_error: 0.0669 - val_acc: 0.9474\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.13619\n",
      "Epoch 9/50\n",
      "4355/4355 [==============================] - 14s 3ms/step - loss: 0.1212 - mean_absolute_error: 0.0686 - acc: 0.9513 - val_loss: 0.1300 - val_mean_absolute_error: 0.0713 - val_acc: 0.9513\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.13619 to 0.12998, saving model to best_model.h5\n",
      "Epoch 10/50\n",
      "4355/4355 [==============================] - 14s 3ms/step - loss: 0.1223 - mean_absolute_error: 0.0691 - acc: 0.9529 - val_loss: 0.1314 - val_mean_absolute_error: 0.0657 - val_acc: 0.9495\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.12998\n",
      "Epoch 11/50\n",
      "4355/4355 [==============================] - 13s 3ms/step - loss: 0.1184 - mean_absolute_error: 0.0687 - acc: 0.9538 - val_loss: 0.1261 - val_mean_absolute_error: 0.0778 - val_acc: 0.9488\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.12998 to 0.12607, saving model to best_model.h5\n",
      "Epoch 12/50\n",
      "4355/4355 [==============================] - 13s 3ms/step - loss: 0.1184 - mean_absolute_error: 0.0672 - acc: 0.9538 - val_loss: 0.1355 - val_mean_absolute_error: 0.0598 - val_acc: 0.9486\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.12607\n",
      "Epoch 13/50\n",
      "4355/4355 [==============================] - 14s 3ms/step - loss: 0.1148 - mean_absolute_error: 0.0619 - acc: 0.9552 - val_loss: 0.1240 - val_mean_absolute_error: 0.0733 - val_acc: 0.9507\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.12607 to 0.12401, saving model to best_model.h5\n",
      "Epoch 14/50\n",
      "4355/4355 [==============================] - 14s 3ms/step - loss: 0.1114 - mean_absolute_error: 0.0628 - acc: 0.9541 - val_loss: 0.1280 - val_mean_absolute_error: 0.0769 - val_acc: 0.9497\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.12401\n",
      "Epoch 15/50\n",
      "4355/4355 [==============================] - 15s 3ms/step - loss: 0.1152 - mean_absolute_error: 0.0653 - acc: 0.9550 - val_loss: 0.1314 - val_mean_absolute_error: 0.0832 - val_acc: 0.9488\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.12401\n",
      "Epoch 16/50\n",
      "4355/4355 [==============================] - 14s 3ms/step - loss: 0.1120 - mean_absolute_error: 0.0646 - acc: 0.9536 - val_loss: 0.1221 - val_mean_absolute_error: 0.0717 - val_acc: 0.9488\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.12401 to 0.12209, saving model to best_model.h5\n",
      "Epoch 17/50\n",
      "4355/4355 [==============================] - 14s 3ms/step - loss: 0.1052 - mean_absolute_error: 0.0648 - acc: 0.9548 - val_loss: 0.1204 - val_mean_absolute_error: 0.0637 - val_acc: 0.9518\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.12209 to 0.12035, saving model to best_model.h5\n",
      "Epoch 18/50\n",
      "4355/4355 [==============================] - 14s 3ms/step - loss: 0.1131 - mean_absolute_error: 0.0650 - acc: 0.9520 - val_loss: 0.1277 - val_mean_absolute_error: 0.0695 - val_acc: 0.9410\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.12035\n",
      "Epoch 19/50\n",
      "4355/4355 [==============================] - 14s 3ms/step - loss: 0.1079 - mean_absolute_error: 0.0645 - acc: 0.9543 - val_loss: 0.1275 - val_mean_absolute_error: 0.0690 - val_acc: 0.9493\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.12035\n",
      "Epoch 20/50\n",
      "4355/4355 [==============================] - 14s 3ms/step - loss: 0.1036 - mean_absolute_error: 0.0607 - acc: 0.9571 - val_loss: 0.1250 - val_mean_absolute_error: 0.0688 - val_acc: 0.9502\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.12035\n",
      "Epoch 21/50\n",
      "4355/4355 [==============================] - 13s 3ms/step - loss: 0.1034 - mean_absolute_error: 0.0600 - acc: 0.9561 - val_loss: 0.1240 - val_mean_absolute_error: 0.0697 - val_acc: 0.9534\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.12035\n",
      "Epoch 22/50\n",
      "4355/4355 [==============================] - 13s 3ms/step - loss: 0.1018 - mean_absolute_error: 0.0603 - acc: 0.9577 - val_loss: 0.1311 - val_mean_absolute_error: 0.0601 - val_acc: 0.9513\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.12035\n",
      "Epoch 23/50\n",
      "4355/4355 [==============================] - 13s 3ms/step - loss: 0.1121 - mean_absolute_error: 0.0642 - acc: 0.9545 - val_loss: 0.1311 - val_mean_absolute_error: 0.0713 - val_acc: 0.9442\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.12035\n",
      "Epoch 24/50\n",
      "4355/4355 [==============================] - 13s 3ms/step - loss: 0.1137 - mean_absolute_error: 0.0692 - acc: 0.9522 - val_loss: 0.1256 - val_mean_absolute_error: 0.0620 - val_acc: 0.9543\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.12035\n",
      "Epoch 25/50\n",
      "4355/4355 [==============================] - 12s 3ms/step - loss: 0.1007 - mean_absolute_error: 0.0555 - acc: 0.9603 - val_loss: 0.1196 - val_mean_absolute_error: 0.0708 - val_acc: 0.9527\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.12035 to 0.11958, saving model to best_model.h5\n",
      "Epoch 26/50\n",
      "4355/4355 [==============================] - 13s 3ms/step - loss: 0.0972 - mean_absolute_error: 0.0596 - acc: 0.9617 - val_loss: 0.1255 - val_mean_absolute_error: 0.0624 - val_acc: 0.9511\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.11958\n",
      "Epoch 27/50\n",
      "4355/4355 [==============================] - 13s 3ms/step - loss: 0.0976 - mean_absolute_error: 0.0557 - acc: 0.9626 - val_loss: 0.1288 - val_mean_absolute_error: 0.0559 - val_acc: 0.9543\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.11958\n",
      "Epoch 28/50\n",
      "4355/4355 [==============================] - 14s 3ms/step - loss: 0.1016 - mean_absolute_error: 0.0609 - acc: 0.9568 - val_loss: 0.1334 - val_mean_absolute_error: 0.0541 - val_acc: 0.9546\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.11958\n",
      "Epoch 29/50\n",
      "4355/4355 [==============================] - 14s 3ms/step - loss: 0.0983 - mean_absolute_error: 0.0575 - acc: 0.9610 - val_loss: 0.1188 - val_mean_absolute_error: 0.0588 - val_acc: 0.9552\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.11958 to 0.11881, saving model to best_model.h5\n",
      "Epoch 30/50\n",
      "4355/4355 [==============================] - 14s 3ms/step - loss: 0.0990 - mean_absolute_error: 0.0576 - acc: 0.9605 - val_loss: 0.1193 - val_mean_absolute_error: 0.0671 - val_acc: 0.9497\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.11881\n",
      "Epoch 31/50\n",
      "4355/4355 [==============================] - 13s 3ms/step - loss: 0.0986 - mean_absolute_error: 0.0570 - acc: 0.9628 - val_loss: 0.1195 - val_mean_absolute_error: 0.0546 - val_acc: 0.9562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00031: val_loss did not improve from 0.11881\n",
      "Epoch 32/50\n",
      "4355/4355 [==============================] - 13s 3ms/step - loss: 0.0994 - mean_absolute_error: 0.0565 - acc: 0.9596 - val_loss: 0.1251 - val_mean_absolute_error: 0.0595 - val_acc: 0.9541\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.11881\n",
      "Epoch 33/50\n",
      "4355/4355 [==============================] - 13s 3ms/step - loss: 0.0979 - mean_absolute_error: 0.0555 - acc: 0.9642 - val_loss: 0.1295 - val_mean_absolute_error: 0.0771 - val_acc: 0.9406\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.11881\n",
      "Epoch 34/50\n",
      "4355/4355 [==============================] - 15s 3ms/step - loss: 0.1374 - mean_absolute_error: 0.0784 - acc: 0.9357 - val_loss: 0.1344 - val_mean_absolute_error: 0.0652 - val_acc: 0.9509\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.11881\n",
      "Epoch 35/50\n",
      "4355/4355 [==============================] - 14s 3ms/step - loss: 0.1098 - mean_absolute_error: 0.0652 - acc: 0.9536 - val_loss: 0.1204 - val_mean_absolute_error: 0.0709 - val_acc: 0.9523\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.11881\n",
      "Epoch 36/50\n",
      "4355/4355 [==============================] - 14s 3ms/step - loss: 0.1044 - mean_absolute_error: 0.0595 - acc: 0.9584 - val_loss: 0.1244 - val_mean_absolute_error: 0.0679 - val_acc: 0.9532\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.11881\n",
      "Epoch 37/50\n",
      "4355/4355 [==============================] - 14s 3ms/step - loss: 0.1130 - mean_absolute_error: 0.0703 - acc: 0.9607 - val_loss: 0.1306 - val_mean_absolute_error: 0.0611 - val_acc: 0.9532\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.11881\n",
      "Epoch 38/50\n",
      "4355/4355 [==============================] - 14s 3ms/step - loss: 0.1013 - mean_absolute_error: 0.0603 - acc: 0.9605 - val_loss: 0.1208 - val_mean_absolute_error: 0.0571 - val_acc: 0.9569\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.11881\n",
      "Epoch 39/50\n",
      "4355/4355 [==============================] - 13s 3ms/step - loss: 0.1044 - mean_absolute_error: 0.0569 - acc: 0.9605 - val_loss: 0.1191 - val_mean_absolute_error: 0.0639 - val_acc: 0.9573\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.11881\n",
      "Epoch 40/50\n",
      "4355/4355 [==============================] - 14s 3ms/step - loss: 0.0978 - mean_absolute_error: 0.0551 - acc: 0.9633 - val_loss: 0.1199 - val_mean_absolute_error: 0.0624 - val_acc: 0.9585\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.11881\n",
      "Epoch 41/50\n",
      "4355/4355 [==============================] - 13s 3ms/step - loss: 0.1150 - mean_absolute_error: 0.0697 - acc: 0.9511 - val_loss: 0.1262 - val_mean_absolute_error: 0.0657 - val_acc: 0.9550\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.11881\n",
      "Epoch 42/50\n",
      "4355/4355 [==============================] - 14s 3ms/step - loss: 0.1060 - mean_absolute_error: 0.0601 - acc: 0.9600 - val_loss: 0.1236 - val_mean_absolute_error: 0.0685 - val_acc: 0.9509\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.11881\n",
      "Epoch 43/50\n",
      "4355/4355 [==============================] - 14s 3ms/step - loss: 0.1033 - mean_absolute_error: 0.0600 - acc: 0.9589 - val_loss: 0.1204 - val_mean_absolute_error: 0.0654 - val_acc: 0.9543\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.11881\n",
      "Epoch 44/50\n",
      "4355/4355 [==============================] - 13s 3ms/step - loss: 0.0960 - mean_absolute_error: 0.0558 - acc: 0.9628 - val_loss: 0.1205 - val_mean_absolute_error: 0.0657 - val_acc: 0.9555\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.11881\n",
      "Epoch 45/50\n",
      "4355/4355 [==============================] - 14s 3ms/step - loss: 0.0932 - mean_absolute_error: 0.0538 - acc: 0.9653 - val_loss: 0.1211 - val_mean_absolute_error: 0.0629 - val_acc: 0.9534\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.11881\n",
      "Epoch 46/50\n",
      "4355/4355 [==============================] - 14s 3ms/step - loss: 0.0950 - mean_absolute_error: 0.0559 - acc: 0.9667 - val_loss: 0.1236 - val_mean_absolute_error: 0.0673 - val_acc: 0.9481\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.11881\n",
      "Epoch 47/50\n",
      "4355/4355 [==============================] - 13s 3ms/step - loss: 0.0931 - mean_absolute_error: 0.0513 - acc: 0.9665 - val_loss: 0.1181 - val_mean_absolute_error: 0.0557 - val_acc: 0.9580\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.11881 to 0.11807, saving model to best_model.h5\n",
      "Epoch 48/50\n",
      "4355/4355 [==============================] - 14s 3ms/step - loss: 0.0977 - mean_absolute_error: 0.0559 - acc: 0.9605 - val_loss: 0.1189 - val_mean_absolute_error: 0.0580 - val_acc: 0.9571\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.11807\n",
      "Epoch 49/50\n",
      "4355/4355 [==============================] - 14s 3ms/step - loss: 0.0862 - mean_absolute_error: 0.0505 - acc: 0.9676 - val_loss: 0.1194 - val_mean_absolute_error: 0.0594 - val_acc: 0.9536\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.11807\n",
      "Epoch 50/50\n",
      "4355/4355 [==============================] - 14s 3ms/step - loss: 0.0988 - mean_absolute_error: 0.0552 - acc: 0.9603 - val_loss: 0.1131 - val_mean_absolute_error: 0.0648 - val_acc: 0.9594\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.11807 to 0.11310, saving model to best_model.h5\n",
      "Beginning fold 2\n",
      "Train on 4357 samples, validate on 4355 samples\n",
      "Epoch 1/50\n",
      "4357/4357 [==============================] - 16s 4ms/step - loss: 0.2481 - mean_absolute_error: 0.1454 - acc: 0.9210 - val_loss: 0.1456 - val_mean_absolute_error: 0.0999 - val_acc: 0.9346\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14564, saving model to best_model.h5\n",
      "Epoch 2/50\n",
      "4357/4357 [==============================] - 14s 3ms/step - loss: 0.1594 - mean_absolute_error: 0.0873 - acc: 0.9403 - val_loss: 0.1857 - val_mean_absolute_error: 0.1428 - val_acc: 0.9495\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.14564\n",
      "Epoch 3/50\n",
      "4357/4357 [==============================] - 13s 3ms/step - loss: 0.1569 - mean_absolute_error: 0.0938 - acc: 0.9422 - val_loss: 0.1460 - val_mean_absolute_error: 0.0886 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.14564\n",
      "Epoch 4/50\n",
      "4357/4357 [==============================] - 13s 3ms/step - loss: 0.1470 - mean_absolute_error: 0.0781 - acc: 0.9429 - val_loss: 0.1257 - val_mean_absolute_error: 0.0787 - val_acc: 0.9488\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.14564 to 0.12573, saving model to best_model.h5\n",
      "Epoch 5/50\n",
      "4357/4357 [==============================] - 13s 3ms/step - loss: 0.1351 - mean_absolute_error: 0.0778 - acc: 0.9445 - val_loss: 0.1233 - val_mean_absolute_error: 0.0673 - val_acc: 0.9518\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.12573 to 0.12331, saving model to best_model.h5\n",
      "Epoch 6/50\n",
      "4357/4357 [==============================] - 13s 3ms/step - loss: 0.1382 - mean_absolute_error: 0.0793 - acc: 0.9456 - val_loss: 0.1248 - val_mean_absolute_error: 0.0698 - val_acc: 0.9490\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.12331\n",
      "Epoch 7/50\n",
      "4357/4357 [==============================] - 14s 3ms/step - loss: 0.1379 - mean_absolute_error: 0.0811 - acc: 0.9364 - val_loss: 0.1161 - val_mean_absolute_error: 0.0657 - val_acc: 0.9536\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.12331 to 0.11610, saving model to best_model.h5\n",
      "Epoch 8/50\n",
      "4357/4357 [==============================] - 14s 3ms/step - loss: 0.1345 - mean_absolute_error: 0.0748 - acc: 0.9435 - val_loss: 0.1150 - val_mean_absolute_error: 0.0702 - val_acc: 0.9499\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.11610 to 0.11505, saving model to best_model.h5\n",
      "Epoch 9/50\n",
      "4357/4357 [==============================] - 13s 3ms/step - loss: 0.1254 - mean_absolute_error: 0.0741 - acc: 0.9470 - val_loss: 0.1140 - val_mean_absolute_error: 0.0672 - val_acc: 0.9568\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.11505 to 0.11400, saving model to best_model.h5\n",
      "Epoch 10/50\n",
      "4357/4357 [==============================] - 13s 3ms/step - loss: 0.1320 - mean_absolute_error: 0.0713 - acc: 0.9490 - val_loss: 0.1232 - val_mean_absolute_error: 0.0823 - val_acc: 0.9536\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.11400\n",
      "Epoch 11/50\n",
      "4357/4357 [==============================] - 14s 3ms/step - loss: 0.1314 - mean_absolute_error: 0.0750 - acc: 0.9504 - val_loss: 0.1172 - val_mean_absolute_error: 0.0660 - val_acc: 0.9538\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.11400\n",
      "Epoch 12/50\n",
      "4357/4357 [==============================] - 14s 3ms/step - loss: 0.1253 - mean_absolute_error: 0.0741 - acc: 0.9484 - val_loss: 0.1118 - val_mean_absolute_error: 0.0670 - val_acc: 0.9543\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.11400 to 0.11176, saving model to best_model.h5\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4357/4357 [==============================] - 14s 3ms/step - loss: 0.1258 - mean_absolute_error: 0.0687 - acc: 0.9497 - val_loss: 0.1127 - val_mean_absolute_error: 0.0701 - val_acc: 0.9548\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.11176\n",
      "Epoch 14/50\n",
      "4357/4357 [==============================] - 14s 3ms/step - loss: 0.1211 - mean_absolute_error: 0.0747 - acc: 0.9490 - val_loss: 0.1148 - val_mean_absolute_error: 0.0732 - val_acc: 0.9536\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.11176\n",
      "Epoch 15/50\n",
      "4357/4357 [==============================] - 14s 3ms/step - loss: 0.1239 - mean_absolute_error: 0.0713 - acc: 0.9490 - val_loss: 0.1190 - val_mean_absolute_error: 0.0653 - val_acc: 0.9456\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.11176\n",
      "Epoch 16/50\n",
      "4357/4357 [==============================] - 14s 3ms/step - loss: 0.1198 - mean_absolute_error: 0.0698 - acc: 0.9477 - val_loss: 0.1095 - val_mean_absolute_error: 0.0624 - val_acc: 0.9559\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.11176 to 0.10951, saving model to best_model.h5\n",
      "Epoch 17/50\n",
      "4357/4357 [==============================] - 14s 3ms/step - loss: 0.1219 - mean_absolute_error: 0.0734 - acc: 0.9481 - val_loss: 0.1118 - val_mean_absolute_error: 0.0666 - val_acc: 0.9522\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.10951\n",
      "Epoch 18/50\n",
      "4357/4357 [==============================] - 14s 3ms/step - loss: 0.1163 - mean_absolute_error: 0.0681 - acc: 0.9493 - val_loss: 0.1119 - val_mean_absolute_error: 0.0638 - val_acc: 0.9552\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.10951\n",
      "Epoch 19/50\n",
      "4357/4357 [==============================] - 15s 3ms/step - loss: 0.1203 - mean_absolute_error: 0.0686 - acc: 0.9507 - val_loss: 0.1090 - val_mean_absolute_error: 0.0696 - val_acc: 0.9541\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.10951 to 0.10897, saving model to best_model.h5\n",
      "Epoch 20/50\n",
      "4357/4357 [==============================] - 14s 3ms/step - loss: 0.1184 - mean_absolute_error: 0.0677 - acc: 0.9481 - val_loss: 0.1188 - val_mean_absolute_error: 0.0584 - val_acc: 0.9518\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.10897\n",
      "Epoch 21/50\n",
      "4357/4357 [==============================] - 14s 3ms/step - loss: 0.1187 - mean_absolute_error: 0.0674 - acc: 0.9497 - val_loss: 0.1098 - val_mean_absolute_error: 0.0599 - val_acc: 0.9575\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.10897\n",
      "Epoch 22/50\n",
      "4357/4357 [==============================] - 14s 3ms/step - loss: 0.1163 - mean_absolute_error: 0.0688 - acc: 0.9527 - val_loss: 0.1104 - val_mean_absolute_error: 0.0710 - val_acc: 0.9534\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.10897\n",
      "Epoch 23/50\n",
      "4357/4357 [==============================] - 15s 3ms/step - loss: 0.1180 - mean_absolute_error: 0.0691 - acc: 0.9516 - val_loss: 0.1304 - val_mean_absolute_error: 0.0583 - val_acc: 0.9428\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.10897\n",
      "Epoch 24/50\n",
      "4357/4357 [==============================] - 14s 3ms/step - loss: 0.1173 - mean_absolute_error: 0.0667 - acc: 0.9509 - val_loss: 0.1092 - val_mean_absolute_error: 0.0598 - val_acc: 0.9582\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.10897\n",
      "Epoch 25/50\n",
      "4357/4357 [==============================] - 13s 3ms/step - loss: 0.1147 - mean_absolute_error: 0.0684 - acc: 0.9548 - val_loss: 0.1133 - val_mean_absolute_error: 0.0592 - val_acc: 0.9548\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.10897\n",
      "Epoch 26/50\n",
      "4357/4357 [==============================] - 13s 3ms/step - loss: 0.1104 - mean_absolute_error: 0.0650 - acc: 0.9573 - val_loss: 0.1225 - val_mean_absolute_error: 0.0843 - val_acc: 0.9598\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.10897\n",
      "Epoch 27/50\n",
      "4357/4357 [==============================] - 14s 3ms/step - loss: 0.1115 - mean_absolute_error: 0.0650 - acc: 0.9557 - val_loss: 0.1069 - val_mean_absolute_error: 0.0634 - val_acc: 0.9573\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.10897 to 0.10692, saving model to best_model.h5\n",
      "Epoch 28/50\n",
      "4357/4357 [==============================] - 14s 3ms/step - loss: 0.1109 - mean_absolute_error: 0.0606 - acc: 0.9575 - val_loss: 0.1104 - val_mean_absolute_error: 0.0571 - val_acc: 0.9561\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.10692\n",
      "Epoch 29/50\n",
      "4357/4357 [==============================] - 14s 3ms/step - loss: 0.1121 - mean_absolute_error: 0.0634 - acc: 0.9573 - val_loss: 0.1056 - val_mean_absolute_error: 0.0554 - val_acc: 0.9582\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.10692 to 0.10558, saving model to best_model.h5\n",
      "Epoch 30/50\n",
      "4357/4357 [==============================] - 14s 3ms/step - loss: 0.1060 - mean_absolute_error: 0.0591 - acc: 0.9628 - val_loss: 0.1069 - val_mean_absolute_error: 0.0649 - val_acc: 0.9552\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.10558\n",
      "Epoch 31/50\n",
      "4357/4357 [==============================] - 14s 3ms/step - loss: 0.1039 - mean_absolute_error: 0.0580 - acc: 0.9642 - val_loss: 0.1087 - val_mean_absolute_error: 0.0612 - val_acc: 0.9580\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.10558\n",
      "Epoch 32/50\n",
      "4357/4357 [==============================] - 14s 3ms/step - loss: 0.1026 - mean_absolute_error: 0.0591 - acc: 0.9630 - val_loss: 0.1038 - val_mean_absolute_error: 0.0554 - val_acc: 0.9587\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.10558 to 0.10383, saving model to best_model.h5\n",
      "Epoch 33/50\n",
      "4357/4357 [==============================] - 13s 3ms/step - loss: 0.1033 - mean_absolute_error: 0.0568 - acc: 0.9610 - val_loss: 0.1034 - val_mean_absolute_error: 0.0561 - val_acc: 0.9598\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.10383 to 0.10337, saving model to best_model.h5\n",
      "Epoch 34/50\n",
      "4357/4357 [==============================] - 14s 3ms/step - loss: 0.0981 - mean_absolute_error: 0.0555 - acc: 0.9633 - val_loss: 0.1034 - val_mean_absolute_error: 0.0580 - val_acc: 0.9577\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.10337 to 0.10336, saving model to best_model.h5\n",
      "Epoch 35/50\n",
      "4357/4357 [==============================] - 13s 3ms/step - loss: 0.0987 - mean_absolute_error: 0.0557 - acc: 0.9628 - val_loss: 0.1050 - val_mean_absolute_error: 0.0526 - val_acc: 0.9587\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.10336\n",
      "Epoch 36/50\n",
      "4357/4357 [==============================] - 13s 3ms/step - loss: 0.0941 - mean_absolute_error: 0.0520 - acc: 0.9651 - val_loss: 0.1215 - val_mean_absolute_error: 0.0724 - val_acc: 0.9490\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.10336\n",
      "Epoch 37/50\n",
      "4357/4357 [==============================] - 14s 3ms/step - loss: 0.0942 - mean_absolute_error: 0.0529 - acc: 0.9656 - val_loss: 0.1028 - val_mean_absolute_error: 0.0560 - val_acc: 0.9617\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.10336 to 0.10276, saving model to best_model.h5\n",
      "Epoch 38/50\n",
      "4357/4357 [==============================] - 14s 3ms/step - loss: 0.1022 - mean_absolute_error: 0.0563 - acc: 0.9610 - val_loss: 0.1022 - val_mean_absolute_error: 0.0579 - val_acc: 0.9603\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.10276 to 0.10221, saving model to best_model.h5\n",
      "Epoch 39/50\n",
      "4357/4357 [==============================] - 14s 3ms/step - loss: 0.0940 - mean_absolute_error: 0.0543 - acc: 0.9669 - val_loss: 0.1000 - val_mean_absolute_error: 0.0563 - val_acc: 0.9614\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.10221 to 0.09997, saving model to best_model.h5\n",
      "Epoch 40/50\n",
      "4357/4357 [==============================] - 15s 3ms/step - loss: 0.0954 - mean_absolute_error: 0.0514 - acc: 0.9672 - val_loss: 0.1054 - val_mean_absolute_error: 0.0625 - val_acc: 0.9577\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.09997\n",
      "Epoch 41/50\n",
      "4357/4357 [==============================] - 14s 3ms/step - loss: 0.0956 - mean_absolute_error: 0.0558 - acc: 0.9656 - val_loss: 0.1035 - val_mean_absolute_error: 0.0455 - val_acc: 0.9642\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.09997\n",
      "Epoch 42/50\n",
      "4357/4357 [==============================] - 14s 3ms/step - loss: 0.0885 - mean_absolute_error: 0.0477 - acc: 0.9706 - val_loss: 0.1011 - val_mean_absolute_error: 0.0496 - val_acc: 0.9628\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.09997\n",
      "Epoch 43/50\n",
      "4357/4357 [==============================] - 13s 3ms/step - loss: 0.0858 - mean_absolute_error: 0.0456 - acc: 0.9697 - val_loss: 0.1117 - val_mean_absolute_error: 0.0550 - val_acc: 0.9603\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.09997\n",
      "Epoch 44/50\n",
      "4357/4357 [==============================] - 14s 3ms/step - loss: 0.0849 - mean_absolute_error: 0.0465 - acc: 0.9697 - val_loss: 0.1040 - val_mean_absolute_error: 0.0474 - val_acc: 0.9628\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.09997\n",
      "Epoch 45/50\n",
      "4357/4357 [==============================] - 14s 3ms/step - loss: 0.0848 - mean_absolute_error: 0.0459 - acc: 0.9711 - val_loss: 0.1054 - val_mean_absolute_error: 0.0472 - val_acc: 0.9635\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.09997\n",
      "Epoch 46/50\n",
      "4357/4357 [==============================] - 14s 3ms/step - loss: 0.0848 - mean_absolute_error: 0.0436 - acc: 0.9727 - val_loss: 0.1022 - val_mean_absolute_error: 0.0514 - val_acc: 0.9626\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.09997\n",
      "Epoch 47/50\n",
      "4357/4357 [==============================] - 14s 3ms/step - loss: 0.0786 - mean_absolute_error: 0.0418 - acc: 0.9722 - val_loss: 0.0970 - val_mean_absolute_error: 0.0535 - val_acc: 0.9633\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.09997 to 0.09696, saving model to best_model.h5\n",
      "Epoch 48/50\n",
      "4357/4357 [==============================] - 14s 3ms/step - loss: 0.0870 - mean_absolute_error: 0.0501 - acc: 0.9669 - val_loss: 0.1006 - val_mean_absolute_error: 0.0552 - val_acc: 0.9598\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.09696\n",
      "Epoch 49/50\n",
      "4357/4357 [==============================] - 14s 3ms/step - loss: 0.0825 - mean_absolute_error: 0.0451 - acc: 0.9683 - val_loss: 0.0967 - val_mean_absolute_error: 0.0565 - val_acc: 0.9621\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.09696 to 0.09672, saving model to best_model.h5\n",
      "Epoch 50/50\n",
      "4357/4357 [==============================] - 14s 3ms/step - loss: 0.0771 - mean_absolute_error: 0.0428 - acc: 0.9729 - val_loss: 0.0988 - val_mean_absolute_error: 0.0555 - val_acc: 0.9617\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.09672\n"
     ]
    }
   ],
   "source": [
    "splits = list(StratifiedKFold(n_splits=2, shuffle=True, random_state=2019).split(X, y))\n",
    "preds_val = []\n",
    "y_val = []\n",
    "for idx, (train_idx, val_idx) in enumerate(splits):\n",
    "    K.clear_session()\n",
    "    print(\"Beginning fold {}\".format(idx+1))\n",
    "    train_X, train_y, val_X, val_y = X[train_idx], y[train_idx], X[val_idx], y[val_idx]\n",
    "    model = create_model(train_X)\n",
    "    monitor = ModelCheckpoint('best_model.h5', save_best_only=True, verbose=1, monitor='val_loss', mode='min')\n",
    "    model.fit(train_X, train_y,validation_data=[val_X, val_y],callbacks=[monitor],batch_size=128,epochs=50)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The function converged at 28 epochs with early stopping.\n",
    "# with model checkpoint at 29th epoch the validation loss stopped improving. in the first fold\n",
    "# 2nd fold at epoch 45\n",
    "#3rd fold at epoch 49\n",
    "# 15 mins approx for each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20337/20337 [==============================] - 26s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# load a saved model\n",
    "from keras.models import load_model\n",
    "# Test data, prediction\n",
    "\n",
    "preds = []\n",
    "for i in range(1):\n",
    "    model = load_model('best_model.h5')\n",
    "    pred = model.predict(X_test, verbose=1)\n",
    "    pred_3 = []\n",
    "    for pred_scalar in pred:\n",
    "        for i in range(3):\n",
    "            pred_3.append(pred_scalar)\n",
    "    preds.append(pred_3)\n",
    "threshold = 0.5\n",
    "preds_test = (np.squeeze(np.mean(preds, axis=0)) > threshold).astype(np.int)\n",
    "# submission['target'] = preds\n",
    "# submission.to_csv('submission_{}.csv'.format(seed), index=False)\n",
    "# submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([0.1041708], dtype=float32),\n",
       "  array([0.1041708], dtype=float32),\n",
       "  array([0.1041708], dtype=float32),\n",
       "  array([0.05182594], dtype=float32),\n",
       "  array([0.05182594], dtype=float32),\n",
       "  array([0.05182594], dtype=float32),\n",
       "  array([0.16023049], dtype=float32),\n",
       "  array([0.16023049], dtype=float32),\n",
       "  array([0.16023049], dtype=float32),\n",
       "  array([0.01475626], dtype=float32),\n",
       "  array([0.01475626], dtype=float32),\n",
       "  array([0.01475626], dtype=float32),\n",
       "  array([0.00903603], dtype=float32),\n",
       "  array([0.00903603], dtype=float32),\n",
       "  array([0.00903603], dtype=float32),\n",
       "  array([0.00253165], dtype=float32),\n",
       "  array([0.00253165], dtype=float32),\n",
       "  array([0.00253165], dtype=float32),\n",
       "  array([0.00270239], dtype=float32),\n",
       "  array([0.00270239], dtype=float32),\n",
       "  array([0.00270239], dtype=float32),\n",
       "  array([0.00298837], dtype=float32),\n",
       "  array([0.00298837], dtype=float32),\n",
       "  array([0.00298837], dtype=float32),\n",
       "  array([0.00362104], dtype=float32),\n",
       "  array([0.00362104], dtype=float32),\n",
       "  array([0.00362104], dtype=float32),\n",
       "  array([0.02414012], dtype=float32),\n",
       "  array([0.02414012], dtype=float32),\n",
       "  array([0.02414012], dtype=float32),\n",
       "  array([0.01245397], dtype=float32),\n",
       "  array([0.01245397], dtype=float32),\n",
       "  array([0.01245397], dtype=float32),\n",
       "  array([0.02638757], dtype=float32),\n",
       "  array([0.02638757], dtype=float32),\n",
       "  array([0.02638757], dtype=float32),\n",
       "  array([0.00305328], dtype=float32),\n",
       "  array([0.00305328], dtype=float32),\n",
       "  array([0.00305328], dtype=float32),\n",
       "  array([0.002983], dtype=float32),\n",
       "  array([0.002983], dtype=float32),\n",
       "  array([0.002983], dtype=float32),\n",
       "  array([0.00246164], dtype=float32),\n",
       "  array([0.00246164], dtype=float32),\n",
       "  array([0.00246164], dtype=float32),\n",
       "  array([0.01010847], dtype=float32),\n",
       "  array([0.01010847], dtype=float32),\n",
       "  array([0.01010847], dtype=float32),\n",
       "  array([0.00165498], dtype=float32),\n",
       "  array([0.00165498], dtype=float32),\n",
       "  array([0.00165498], dtype=float32),\n",
       "  array([0.00388831], dtype=float32),\n",
       "  array([0.00388831], dtype=float32),\n",
       "  array([0.00388831], dtype=float32),\n",
       "  array([0.00301504], dtype=float32),\n",
       "  array([0.00301504], dtype=float32),\n",
       "  array([0.00301504], dtype=float32),\n",
       "  array([0.00431332], dtype=float32),\n",
       "  array([0.00431332], dtype=float32),\n",
       "  array([0.00431332], dtype=float32),\n",
       "  array([0.00963205], dtype=float32),\n",
       "  array([0.00963205], dtype=float32),\n",
       "  array([0.00963205], dtype=float32),\n",
       "  array([0.00537804], dtype=float32),\n",
       "  array([0.00537804], dtype=float32),\n",
       "  array([0.00537804], dtype=float32),\n",
       "  array([0.0035778], dtype=float32),\n",
       "  array([0.0035778], dtype=float32),\n",
       "  array([0.0035778], dtype=float32),\n",
       "  array([0.00752842], dtype=float32),\n",
       "  array([0.00752842], dtype=float32),\n",
       "  array([0.00752842], dtype=float32),\n",
       "  array([0.01524118], dtype=float32),\n",
       "  array([0.01524118], dtype=float32),\n",
       "  array([0.01524118], dtype=float32),\n",
       "  array([0.00779065], dtype=float32),\n",
       "  array([0.00779065], dtype=float32),\n",
       "  array([0.00779065], dtype=float32),\n",
       "  array([0.06463119], dtype=float32),\n",
       "  array([0.06463119], dtype=float32),\n",
       "  array([0.06463119], dtype=float32),\n",
       "  array([0.00129545], dtype=float32),\n",
       "  array([0.00129545], dtype=float32),\n",
       "  array([0.00129545], dtype=float32),\n",
       "  array([0.00132525], dtype=float32),\n",
       "  array([0.00132525], dtype=float32),\n",
       "  array([0.00132525], dtype=float32),\n",
       "  array([0.001196], dtype=float32),\n",
       "  array([0.001196], dtype=float32),\n",
       "  array([0.001196], dtype=float32),\n",
       "  array([0.00218305], dtype=float32),\n",
       "  array([0.00218305], dtype=float32),\n",
       "  array([0.00218305], dtype=float32),\n",
       "  array([0.00255328], dtype=float32),\n",
       "  array([0.00255328], dtype=float32),\n",
       "  array([0.00255328], dtype=float32),\n",
       "  array([0.00129822], dtype=float32),\n",
       "  array([0.00129822], dtype=float32),\n",
       "  array([0.00129822], dtype=float32),\n",
       "  array([0.00132999], dtype=float32),\n",
       "  array([0.00132999], dtype=float32),\n",
       "  array([0.00132999], dtype=float32),\n",
       "  array([0.00262734], dtype=float32),\n",
       "  array([0.00262734], dtype=float32),\n",
       "  array([0.00262734], dtype=float32),\n",
       "  array([0.00131997], dtype=float32),\n",
       "  array([0.00131997], dtype=float32),\n",
       "  array([0.00131997], dtype=float32),\n",
       "  array([0.61924624], dtype=float32),\n",
       "  array([0.61924624], dtype=float32),\n",
       "  array([0.61924624], dtype=float32),\n",
       "  array([0.99841], dtype=float32),\n",
       "  array([0.99841], dtype=float32),\n",
       "  array([0.99841], dtype=float32),\n",
       "  array([0.7831045], dtype=float32),\n",
       "  array([0.7831045], dtype=float32),\n",
       "  array([0.7831045], dtype=float32),\n",
       "  array([0.00461939], dtype=float32),\n",
       "  array([0.00461939], dtype=float32),\n",
       "  array([0.00461939], dtype=float32),\n",
       "  array([0.00423121], dtype=float32),\n",
       "  array([0.00423121], dtype=float32),\n",
       "  array([0.00423121], dtype=float32),\n",
       "  array([0.00430343], dtype=float32),\n",
       "  array([0.00430343], dtype=float32),\n",
       "  array([0.00430343], dtype=float32),\n",
       "  array([0.6548572], dtype=float32),\n",
       "  array([0.6548572], dtype=float32),\n",
       "  array([0.6548572], dtype=float32),\n",
       "  array([0.17443654], dtype=float32),\n",
       "  array([0.17443654], dtype=float32),\n",
       "  array([0.17443654], dtype=float32),\n",
       "  array([0.643137], dtype=float32),\n",
       "  array([0.643137], dtype=float32),\n",
       "  array([0.643137], dtype=float32),\n",
       "  array([0.00226334], dtype=float32),\n",
       "  array([0.00226334], dtype=float32),\n",
       "  array([0.00226334], dtype=float32),\n",
       "  array([0.00254506], dtype=float32),\n",
       "  array([0.00254506], dtype=float32),\n",
       "  array([0.00254506], dtype=float32),\n",
       "  array([0.00199607], dtype=float32),\n",
       "  array([0.00199607], dtype=float32),\n",
       "  array([0.00199607], dtype=float32),\n",
       "  array([0.00566244], dtype=float32),\n",
       "  array([0.00566244], dtype=float32),\n",
       "  array([0.00566244], dtype=float32),\n",
       "  array([0.00834882], dtype=float32),\n",
       "  array([0.00834882], dtype=float32),\n",
       "  array([0.00834882], dtype=float32),\n",
       "  array([0.0044091], dtype=float32),\n",
       "  array([0.0044091], dtype=float32),\n",
       "  array([0.0044091], dtype=float32),\n",
       "  array([0.00386533], dtype=float32),\n",
       "  array([0.00386533], dtype=float32),\n",
       "  array([0.00386533], dtype=float32),\n",
       "  array([0.00743765], dtype=float32),\n",
       "  array([0.00743765], dtype=float32),\n",
       "  array([0.00743765], dtype=float32),\n",
       "  array([0.00589749], dtype=float32),\n",
       "  array([0.00589749], dtype=float32),\n",
       "  array([0.00589749], dtype=float32),\n",
       "  array([0.01055712], dtype=float32),\n",
       "  array([0.01055712], dtype=float32),\n",
       "  array([0.01055712], dtype=float32),\n",
       "  array([0.00927722], dtype=float32),\n",
       "  array([0.00927722], dtype=float32),\n",
       "  array([0.00927722], dtype=float32),\n",
       "  array([0.00626379], dtype=float32),\n",
       "  array([0.00626379], dtype=float32),\n",
       "  array([0.00626379], dtype=float32),\n",
       "  array([0.21161082], dtype=float32),\n",
       "  array([0.21161082], dtype=float32),\n",
       "  array([0.21161082], dtype=float32),\n",
       "  array([0.02702364], dtype=float32),\n",
       "  array([0.02702364], dtype=float32),\n",
       "  array([0.02702364], dtype=float32),\n",
       "  array([0.15184623], dtype=float32),\n",
       "  array([0.15184623], dtype=float32),\n",
       "  array([0.15184623], dtype=float32),\n",
       "  array([0.01509556], dtype=float32),\n",
       "  array([0.01509556], dtype=float32),\n",
       "  array([0.01509556], dtype=float32),\n",
       "  array([0.00415534], dtype=float32),\n",
       "  array([0.00415534], dtype=float32),\n",
       "  array([0.00415534], dtype=float32),\n",
       "  array([0.00390261], dtype=float32),\n",
       "  array([0.00390261], dtype=float32),\n",
       "  array([0.00390261], dtype=float32),\n",
       "  array([0.0058291], dtype=float32),\n",
       "  array([0.0058291], dtype=float32),\n",
       "  array([0.0058291], dtype=float32),\n",
       "  array([0.00614983], dtype=float32),\n",
       "  array([0.00614983], dtype=float32),\n",
       "  array([0.00614983], dtype=float32),\n",
       "  array([0.0100325], dtype=float32),\n",
       "  array([0.0100325], dtype=float32),\n",
       "  array([0.0100325], dtype=float32),\n",
       "  array([0.00389928], dtype=float32),\n",
       "  array([0.00389928], dtype=float32),\n",
       "  array([0.00389928], dtype=float32),\n",
       "  array([0.00399515], dtype=float32),\n",
       "  array([0.00399515], dtype=float32),\n",
       "  array([0.00399515], dtype=float32),\n",
       "  array([0.00469223], dtype=float32),\n",
       "  array([0.00469223], dtype=float32),\n",
       "  array([0.00469223], dtype=float32),\n",
       "  array([0.01490358], dtype=float32),\n",
       "  array([0.01490358], dtype=float32),\n",
       "  array([0.01490358], dtype=float32),\n",
       "  array([0.01806557], dtype=float32),\n",
       "  array([0.01806557], dtype=float32),\n",
       "  array([0.01806557], dtype=float32),\n",
       "  array([0.01554355], dtype=float32),\n",
       "  array([0.01554355], dtype=float32),\n",
       "  array([0.01554355], dtype=float32),\n",
       "  array([0.0023855], dtype=float32),\n",
       "  array([0.0023855], dtype=float32),\n",
       "  array([0.0023855], dtype=float32),\n",
       "  array([0.01491079], dtype=float32),\n",
       "  array([0.01491079], dtype=float32),\n",
       "  array([0.01491079], dtype=float32),\n",
       "  array([0.00971568], dtype=float32),\n",
       "  array([0.00971568], dtype=float32),\n",
       "  array([0.00971568], dtype=float32),\n",
       "  array([0.00257272], dtype=float32),\n",
       "  array([0.00257272], dtype=float32),\n",
       "  array([0.00257272], dtype=float32),\n",
       "  array([0.00235149], dtype=float32),\n",
       "  array([0.00235149], dtype=float32),\n",
       "  array([0.00235149], dtype=float32),\n",
       "  array([0.0025239], dtype=float32),\n",
       "  array([0.0025239], dtype=float32),\n",
       "  array([0.0025239], dtype=float32),\n",
       "  array([0.00247863], dtype=float32),\n",
       "  array([0.00247863], dtype=float32),\n",
       "  array([0.00247863], dtype=float32),\n",
       "  array([0.00240347], dtype=float32),\n",
       "  array([0.00240347], dtype=float32),\n",
       "  array([0.00240347], dtype=float32),\n",
       "  array([0.00242698], dtype=float32),\n",
       "  array([0.00242698], dtype=float32),\n",
       "  array([0.00242698], dtype=float32),\n",
       "  array([0.0021486], dtype=float32),\n",
       "  array([0.0021486], dtype=float32),\n",
       "  array([0.0021486], dtype=float32),\n",
       "  array([0.0027675], dtype=float32),\n",
       "  array([0.0027675], dtype=float32),\n",
       "  array([0.0027675], dtype=float32),\n",
       "  array([0.00128463], dtype=float32),\n",
       "  array([0.00128463], dtype=float32),\n",
       "  array([0.00128463], dtype=float32),\n",
       "  array([0.0033868], dtype=float32),\n",
       "  array([0.0033868], dtype=float32),\n",
       "  array([0.0033868], dtype=float32),\n",
       "  array([0.00328153], dtype=float32),\n",
       "  array([0.00328153], dtype=float32),\n",
       "  array([0.00328153], dtype=float32),\n",
       "  array([0.00283995], dtype=float32),\n",
       "  array([0.00283995], dtype=float32),\n",
       "  array([0.00283995], dtype=float32),\n",
       "  array([0.00339186], dtype=float32),\n",
       "  array([0.00339186], dtype=float32),\n",
       "  array([0.00339186], dtype=float32),\n",
       "  array([0.0031904], dtype=float32),\n",
       "  array([0.0031904], dtype=float32),\n",
       "  array([0.0031904], dtype=float32),\n",
       "  array([0.00249264], dtype=float32),\n",
       "  array([0.00249264], dtype=float32),\n",
       "  array([0.00249264], dtype=float32),\n",
       "  array([0.0052163], dtype=float32),\n",
       "  array([0.0052163], dtype=float32),\n",
       "  array([0.0052163], dtype=float32),\n",
       "  array([0.00394756], dtype=float32),\n",
       "  array([0.00394756], dtype=float32),\n",
       "  array([0.00394756], dtype=float32),\n",
       "  array([0.00526661], dtype=float32),\n",
       "  array([0.00526661], dtype=float32),\n",
       "  array([0.00526661], dtype=float32),\n",
       "  array([0.06090772], dtype=float32),\n",
       "  array([0.06090772], dtype=float32),\n",
       "  array([0.06090772], dtype=float32),\n",
       "  array([0.02031636], dtype=float32),\n",
       "  array([0.02031636], dtype=float32),\n",
       "  array([0.02031636], dtype=float32),\n",
       "  array([0.06769156], dtype=float32),\n",
       "  array([0.06769156], dtype=float32),\n",
       "  array([0.06769156], dtype=float32),\n",
       "  array([0.01693162], dtype=float32),\n",
       "  array([0.01693162], dtype=float32),\n",
       "  array([0.01693162], dtype=float32),\n",
       "  array([0.01441565], dtype=float32),\n",
       "  array([0.01441565], dtype=float32),\n",
       "  array([0.01441565], dtype=float32),\n",
       "  array([0.00933802], dtype=float32),\n",
       "  array([0.00933802], dtype=float32),\n",
       "  array([0.00933802], dtype=float32),\n",
       "  array([0.00252607], dtype=float32),\n",
       "  array([0.00252607], dtype=float32),\n",
       "  array([0.00252607], dtype=float32),\n",
       "  array([0.00249785], dtype=float32),\n",
       "  array([0.00249785], dtype=float32),\n",
       "  array([0.00249785], dtype=float32),\n",
       "  array([0.00245923], dtype=float32),\n",
       "  array([0.00245923], dtype=float32),\n",
       "  array([0.00245923], dtype=float32),\n",
       "  array([0.00237665], dtype=float32),\n",
       "  array([0.00237665], dtype=float32),\n",
       "  array([0.00237665], dtype=float32),\n",
       "  array([0.00461495], dtype=float32),\n",
       "  array([0.00461495], dtype=float32),\n",
       "  array([0.00461495], dtype=float32),\n",
       "  array([0.00288919], dtype=float32),\n",
       "  array([0.00288919], dtype=float32),\n",
       "  array([0.00288919], dtype=float32),\n",
       "  array([0.00193852], dtype=float32),\n",
       "  array([0.00193852], dtype=float32),\n",
       "  array([0.00193852], dtype=float32),\n",
       "  array([0.00113779], dtype=float32),\n",
       "  array([0.00113779], dtype=float32),\n",
       "  array([0.00113779], dtype=float32),\n",
       "  array([0.00138402], dtype=float32),\n",
       "  array([0.00138402], dtype=float32),\n",
       "  array([0.00138402], dtype=float32),\n",
       "  array([0.17138854], dtype=float32),\n",
       "  array([0.17138854], dtype=float32),\n",
       "  array([0.17138854], dtype=float32),\n",
       "  array([0.04849207], dtype=float32),\n",
       "  array([0.04849207], dtype=float32),\n",
       "  array([0.04849207], dtype=float32),\n",
       "  array([0.12509745], dtype=float32),\n",
       "  array([0.12509745], dtype=float32),\n",
       "  array([0.12509745], dtype=float32),\n",
       "  array([0.00110778], dtype=float32),\n",
       "  array([0.00110778], dtype=float32),\n",
       "  array([0.00110778], dtype=float32),\n",
       "  array([0.00115165], dtype=float32),\n",
       "  array([0.00115165], dtype=float32),\n",
       "  array([0.00115165], dtype=float32),\n",
       "  array([0.01449609], dtype=float32),\n",
       "  array([0.01449609], dtype=float32),\n",
       "  array([0.01449609], dtype=float32),\n",
       "  array([0.00764245], dtype=float32),\n",
       "  array([0.00764245], dtype=float32),\n",
       "  array([0.00764245], dtype=float32),\n",
       "  array([0.00395891], dtype=float32),\n",
       "  array([0.00395891], dtype=float32),\n",
       "  array([0.00395891], dtype=float32),\n",
       "  array([0.00639483], dtype=float32),\n",
       "  array([0.00639483], dtype=float32),\n",
       "  array([0.00639483], dtype=float32),\n",
       "  array([0.00403747], dtype=float32),\n",
       "  array([0.00403747], dtype=float32),\n",
       "  array([0.00403747], dtype=float32),\n",
       "  array([0.0075601], dtype=float32),\n",
       "  array([0.0075601], dtype=float32),\n",
       "  array([0.0075601], dtype=float32),\n",
       "  array([0.00754559], dtype=float32),\n",
       "  array([0.00754559], dtype=float32),\n",
       "  array([0.00754559], dtype=float32),\n",
       "  array([0.00132957], dtype=float32),\n",
       "  array([0.00132957], dtype=float32),\n",
       "  array([0.00132957], dtype=float32),\n",
       "  array([0.00212243], dtype=float32),\n",
       "  array([0.00212243], dtype=float32),\n",
       "  array([0.00212243], dtype=float32),\n",
       "  array([0.00132763], dtype=float32),\n",
       "  array([0.00132763], dtype=float32),\n",
       "  array([0.00132763], dtype=float32),\n",
       "  array([0.00448918], dtype=float32),\n",
       "  array([0.00448918], dtype=float32),\n",
       "  array([0.00448918], dtype=float32),\n",
       "  array([0.00395942], dtype=float32),\n",
       "  array([0.00395942], dtype=float32),\n",
       "  array([0.00395942], dtype=float32),\n",
       "  array([0.00376529], dtype=float32),\n",
       "  array([0.00376529], dtype=float32),\n",
       "  array([0.00376529], dtype=float32),\n",
       "  array([0.01494345], dtype=float32),\n",
       "  array([0.01494345], dtype=float32),\n",
       "  array([0.01494345], dtype=float32),\n",
       "  array([0.01065627], dtype=float32),\n",
       "  array([0.01065627], dtype=float32),\n",
       "  array([0.01065627], dtype=float32),\n",
       "  array([0.01445869], dtype=float32),\n",
       "  array([0.01445869], dtype=float32),\n",
       "  array([0.01445869], dtype=float32),\n",
       "  array([0.03738979], dtype=float32),\n",
       "  array([0.03738979], dtype=float32),\n",
       "  array([0.03738979], dtype=float32),\n",
       "  array([0.01476732], dtype=float32),\n",
       "  array([0.01476732], dtype=float32),\n",
       "  array([0.01476732], dtype=float32),\n",
       "  array([0.01356432], dtype=float32),\n",
       "  array([0.01356432], dtype=float32),\n",
       "  array([0.01356432], dtype=float32),\n",
       "  array([0.00299013], dtype=float32),\n",
       "  array([0.00299013], dtype=float32),\n",
       "  array([0.00299013], dtype=float32),\n",
       "  array([0.0025171], dtype=float32),\n",
       "  array([0.0025171], dtype=float32),\n",
       "  array([0.0025171], dtype=float32),\n",
       "  array([0.00213537], dtype=float32),\n",
       "  array([0.00213537], dtype=float32),\n",
       "  array([0.00213537], dtype=float32),\n",
       "  array([0.0025464], dtype=float32),\n",
       "  array([0.0025464], dtype=float32),\n",
       "  array([0.0025464], dtype=float32),\n",
       "  array([0.00244862], dtype=float32),\n",
       "  array([0.00244862], dtype=float32),\n",
       "  array([0.00244862], dtype=float32),\n",
       "  array([0.0023545], dtype=float32),\n",
       "  array([0.0023545], dtype=float32),\n",
       "  array([0.0023545], dtype=float32),\n",
       "  array([0.00928858], dtype=float32),\n",
       "  array([0.00928858], dtype=float32),\n",
       "  array([0.00928858], dtype=float32),\n",
       "  array([0.00282875], dtype=float32),\n",
       "  array([0.00282875], dtype=float32),\n",
       "  array([0.00282875], dtype=float32),\n",
       "  array([0.00524628], dtype=float32),\n",
       "  array([0.00524628], dtype=float32),\n",
       "  array([0.00524628], dtype=float32),\n",
       "  array([0.23022327], dtype=float32),\n",
       "  array([0.23022327], dtype=float32),\n",
       "  array([0.23022327], dtype=float32),\n",
       "  array([0.02570802], dtype=float32),\n",
       "  array([0.02570802], dtype=float32),\n",
       "  array([0.02570802], dtype=float32),\n",
       "  array([0.14526516], dtype=float32),\n",
       "  array([0.14526516], dtype=float32),\n",
       "  array([0.14526516], dtype=float32),\n",
       "  array([0.00334489], dtype=float32),\n",
       "  array([0.00334489], dtype=float32),\n",
       "  array([0.00334489], dtype=float32),\n",
       "  array([0.00149226], dtype=float32),\n",
       "  array([0.00149226], dtype=float32),\n",
       "  array([0.00149226], dtype=float32),\n",
       "  array([0.01071241], dtype=float32),\n",
       "  array([0.01071241], dtype=float32),\n",
       "  array([0.01071241], dtype=float32),\n",
       "  array([0.00329697], dtype=float32),\n",
       "  array([0.00329697], dtype=float32),\n",
       "  array([0.00329697], dtype=float32),\n",
       "  array([0.00728387], dtype=float32),\n",
       "  array([0.00728387], dtype=float32),\n",
       "  array([0.00728387], dtype=float32),\n",
       "  array([0.01137882], dtype=float32),\n",
       "  array([0.01137882], dtype=float32),\n",
       "  array([0.01137882], dtype=float32),\n",
       "  array([0.00714523], dtype=float32),\n",
       "  array([0.00714523], dtype=float32),\n",
       "  array([0.00714523], dtype=float32),\n",
       "  array([0.00680265], dtype=float32),\n",
       "  array([0.00680265], dtype=float32),\n",
       "  array([0.00680265], dtype=float32),\n",
       "  array([0.01281244], dtype=float32),\n",
       "  array([0.01281244], dtype=float32),\n",
       "  array([0.01281244], dtype=float32),\n",
       "  array([0.17613447], dtype=float32),\n",
       "  array([0.17613447], dtype=float32),\n",
       "  array([0.17613447], dtype=float32),\n",
       "  array([0.13548517], dtype=float32),\n",
       "  array([0.13548517], dtype=float32),\n",
       "  array([0.13548517], dtype=float32),\n",
       "  array([0.17998347], dtype=float32),\n",
       "  array([0.17998347], dtype=float32),\n",
       "  array([0.17998347], dtype=float32),\n",
       "  array([0.00418916], dtype=float32),\n",
       "  array([0.00418916], dtype=float32),\n",
       "  array([0.00418916], dtype=float32),\n",
       "  array([0.00338054], dtype=float32),\n",
       "  array([0.00338054], dtype=float32),\n",
       "  array([0.00338054], dtype=float32),\n",
       "  array([0.00370428], dtype=float32),\n",
       "  array([0.00370428], dtype=float32),\n",
       "  array([0.00370428], dtype=float32),\n",
       "  array([0.00348043], dtype=float32),\n",
       "  array([0.00348043], dtype=float32),\n",
       "  array([0.00348043], dtype=float32),\n",
       "  array([0.00353077], dtype=float32),\n",
       "  array([0.00353077], dtype=float32),\n",
       "  array([0.00353077], dtype=float32),\n",
       "  array([0.00326738], dtype=float32),\n",
       "  array([0.00326738], dtype=float32),\n",
       "  array([0.00326738], dtype=float32),\n",
       "  array([0.00261259], dtype=float32),\n",
       "  array([0.00261259], dtype=float32),\n",
       "  array([0.00261259], dtype=float32),\n",
       "  array([0.90108716], dtype=float32),\n",
       "  array([0.90108716], dtype=float32),\n",
       "  array([0.90108716], dtype=float32),\n",
       "  array([0.00351399], dtype=float32),\n",
       "  array([0.00351399], dtype=float32),\n",
       "  array([0.00351399], dtype=float32),\n",
       "  array([0.00127903], dtype=float32),\n",
       "  array([0.00127903], dtype=float32),\n",
       "  array([0.00127903], dtype=float32),\n",
       "  array([0.00124529], dtype=float32),\n",
       "  array([0.00124529], dtype=float32),\n",
       "  array([0.00124529], dtype=float32),\n",
       "  array([0.00160667], dtype=float32),\n",
       "  array([0.00160667], dtype=float32),\n",
       "  array([0.00160667], dtype=float32),\n",
       "  array([0.01470479], dtype=float32),\n",
       "  array([0.01470479], dtype=float32),\n",
       "  array([0.01470479], dtype=float32),\n",
       "  array([0.00842303], dtype=float32),\n",
       "  array([0.00842303], dtype=float32),\n",
       "  array([0.00842303], dtype=float32),\n",
       "  array([0.01472482], dtype=float32),\n",
       "  array([0.01472482], dtype=float32),\n",
       "  array([0.01472482], dtype=float32),\n",
       "  array([0.22846326], dtype=float32),\n",
       "  array([0.22846326], dtype=float32),\n",
       "  array([0.22846326], dtype=float32),\n",
       "  array([0.04525068], dtype=float32),\n",
       "  array([0.04525068], dtype=float32),\n",
       "  array([0.04525068], dtype=float32),\n",
       "  array([0.18139422], dtype=float32),\n",
       "  array([0.18139422], dtype=float32),\n",
       "  array([0.18139422], dtype=float32),\n",
       "  array([0.00305447], dtype=float32),\n",
       "  array([0.00305447], dtype=float32),\n",
       "  array([0.00305447], dtype=float32),\n",
       "  array([0.0030773], dtype=float32),\n",
       "  array([0.0030773], dtype=float32),\n",
       "  array([0.0030773], dtype=float32),\n",
       "  array([0.00259247], dtype=float32),\n",
       "  array([0.00259247], dtype=float32),\n",
       "  array([0.00259247], dtype=float32),\n",
       "  array([0.00124428], dtype=float32),\n",
       "  array([0.00124428], dtype=float32),\n",
       "  array([0.00124428], dtype=float32),\n",
       "  array([0.00116822], dtype=float32),\n",
       "  array([0.00116822], dtype=float32),\n",
       "  array([0.00116822], dtype=float32),\n",
       "  array([0.00159898], dtype=float32),\n",
       "  array([0.00159898], dtype=float32),\n",
       "  array([0.00159898], dtype=float32),\n",
       "  array([0.00146401], dtype=float32),\n",
       "  array([0.00146401], dtype=float32),\n",
       "  array([0.00146401], dtype=float32),\n",
       "  array([0.00137541], dtype=float32),\n",
       "  array([0.00137541], dtype=float32),\n",
       "  array([0.00137541], dtype=float32),\n",
       "  array([0.00208393], dtype=float32),\n",
       "  array([0.00208393], dtype=float32),\n",
       "  array([0.00208393], dtype=float32),\n",
       "  array([0.0038842], dtype=float32),\n",
       "  array([0.0038842], dtype=float32),\n",
       "  array([0.0038842], dtype=float32),\n",
       "  array([0.00458592], dtype=float32),\n",
       "  array([0.00458592], dtype=float32),\n",
       "  array([0.00458592], dtype=float32),\n",
       "  array([0.00435814], dtype=float32),\n",
       "  array([0.00435814], dtype=float32),\n",
       "  array([0.00435814], dtype=float32),\n",
       "  array([0.00705576], dtype=float32),\n",
       "  array([0.00705576], dtype=float32),\n",
       "  array([0.00705576], dtype=float32),\n",
       "  array([0.00622123], dtype=float32),\n",
       "  array([0.00622123], dtype=float32),\n",
       "  array([0.00622123], dtype=float32),\n",
       "  array([0.01113057], dtype=float32),\n",
       "  array([0.01113057], dtype=float32),\n",
       "  array([0.01113057], dtype=float32),\n",
       "  array([0.47687602], dtype=float32),\n",
       "  array([0.47687602], dtype=float32),\n",
       "  array([0.47687602], dtype=float32),\n",
       "  array([0.48048908], dtype=float32),\n",
       "  array([0.48048908], dtype=float32),\n",
       "  array([0.48048908], dtype=float32),\n",
       "  array([0.55906117], dtype=float32),\n",
       "  array([0.55906117], dtype=float32),\n",
       "  array([0.55906117], dtype=float32),\n",
       "  array([0.00391281], dtype=float32),\n",
       "  array([0.00391281], dtype=float32),\n",
       "  array([0.00391281], dtype=float32),\n",
       "  array([0.00730899], dtype=float32),\n",
       "  array([0.00730899], dtype=float32),\n",
       "  array([0.00730899], dtype=float32),\n",
       "  array([0.00315303], dtype=float32),\n",
       "  array([0.00315303], dtype=float32),\n",
       "  array([0.00315303], dtype=float32),\n",
       "  array([0.00537875], dtype=float32),\n",
       "  array([0.00537875], dtype=float32),\n",
       "  array([0.00537875], dtype=float32),\n",
       "  array([0.39516783], dtype=float32),\n",
       "  array([0.39516783], dtype=float32),\n",
       "  array([0.39516783], dtype=float32),\n",
       "  array([0.01456109], dtype=float32),\n",
       "  array([0.01456109], dtype=float32),\n",
       "  array([0.01456109], dtype=float32),\n",
       "  array([0.00259948], dtype=float32),\n",
       "  array([0.00259948], dtype=float32),\n",
       "  array([0.00259948], dtype=float32),\n",
       "  array([0.00265482], dtype=float32),\n",
       "  array([0.00265482], dtype=float32),\n",
       "  array([0.00265482], dtype=float32),\n",
       "  array([0.00210664], dtype=float32),\n",
       "  array([0.00210664], dtype=float32),\n",
       "  array([0.00210664], dtype=float32),\n",
       "  array([0.00241771], dtype=float32),\n",
       "  array([0.00241771], dtype=float32),\n",
       "  array([0.00241771], dtype=float32),\n",
       "  array([0.00256863], dtype=float32),\n",
       "  array([0.00256863], dtype=float32),\n",
       "  array([0.00256863], dtype=float32),\n",
       "  array([0.00138223], dtype=float32),\n",
       "  array([0.00138223], dtype=float32),\n",
       "  array([0.00138223], dtype=float32),\n",
       "  array([0.00879493], dtype=float32),\n",
       "  array([0.00879493], dtype=float32),\n",
       "  array([0.00879493], dtype=float32),\n",
       "  array([0.01080126], dtype=float32),\n",
       "  array([0.01080126], dtype=float32),\n",
       "  array([0.01080126], dtype=float32),\n",
       "  array([0.00517759], dtype=float32),\n",
       "  array([0.00517759], dtype=float32),\n",
       "  array([0.00517759], dtype=float32),\n",
       "  array([0.083112], dtype=float32),\n",
       "  array([0.083112], dtype=float32),\n",
       "  array([0.083112], dtype=float32),\n",
       "  array([0.01366308], dtype=float32),\n",
       "  array([0.01366308], dtype=float32),\n",
       "  array([0.01366308], dtype=float32),\n",
       "  array([0.03505385], dtype=float32),\n",
       "  array([0.03505385], dtype=float32),\n",
       "  array([0.03505385], dtype=float32),\n",
       "  array([0.00131708], dtype=float32),\n",
       "  array([0.00131708], dtype=float32),\n",
       "  array([0.00131708], dtype=float32),\n",
       "  array([0.00120094], dtype=float32),\n",
       "  array([0.00120094], dtype=float32),\n",
       "  array([0.00120094], dtype=float32),\n",
       "  array([0.00173813], dtype=float32),\n",
       "  array([0.00173813], dtype=float32),\n",
       "  array([0.00173813], dtype=float32),\n",
       "  array([0.00332686], dtype=float32),\n",
       "  array([0.00332686], dtype=float32),\n",
       "  array([0.00332686], dtype=float32),\n",
       "  array([0.00356635], dtype=float32),\n",
       "  array([0.00356635], dtype=float32),\n",
       "  array([0.00356635], dtype=float32),\n",
       "  array([0.04944599], dtype=float32),\n",
       "  array([0.04944599], dtype=float32),\n",
       "  array([0.04944599], dtype=float32),\n",
       "  array([0.00276724], dtype=float32),\n",
       "  array([0.00276724], dtype=float32),\n",
       "  array([0.00276724], dtype=float32),\n",
       "  array([0.00187179], dtype=float32),\n",
       "  array([0.00187179], dtype=float32),\n",
       "  array([0.00187179], dtype=float32),\n",
       "  array([0.01465365], dtype=float32),\n",
       "  array([0.01465365], dtype=float32),\n",
       "  array([0.01465365], dtype=float32),\n",
       "  array([0.00586906], dtype=float32),\n",
       "  array([0.00586906], dtype=float32),\n",
       "  array([0.00586906], dtype=float32),\n",
       "  array([0.00229314], dtype=float32),\n",
       "  array([0.00229314], dtype=float32),\n",
       "  array([0.00229314], dtype=float32),\n",
       "  array([0.00884616], dtype=float32),\n",
       "  array([0.00884616], dtype=float32),\n",
       "  array([0.00884616], dtype=float32),\n",
       "  array([0.00253466], dtype=float32),\n",
       "  array([0.00253466], dtype=float32),\n",
       "  array([0.00253466], dtype=float32),\n",
       "  array([0.82739234], dtype=float32),\n",
       "  array([0.82739234], dtype=float32),\n",
       "  array([0.82739234], dtype=float32),\n",
       "  array([0.00281128], dtype=float32),\n",
       "  array([0.00281128], dtype=float32),\n",
       "  array([0.00281128], dtype=float32),\n",
       "  array([0.00335529], dtype=float32),\n",
       "  array([0.00335529], dtype=float32),\n",
       "  array([0.00335529], dtype=float32),\n",
       "  array([0.00371435], dtype=float32),\n",
       "  array([0.00371435], dtype=float32),\n",
       "  array([0.00371435], dtype=float32),\n",
       "  array([0.00328162], dtype=float32),\n",
       "  array([0.00328162], dtype=float32),\n",
       "  array([0.00328162], dtype=float32),\n",
       "  array([0.0893845], dtype=float32),\n",
       "  array([0.0893845], dtype=float32),\n",
       "  array([0.0893845], dtype=float32),\n",
       "  array([0.01779106], dtype=float32),\n",
       "  array([0.01779106], dtype=float32),\n",
       "  array([0.01779106], dtype=float32),\n",
       "  array([0.030435], dtype=float32),\n",
       "  array([0.030435], dtype=float32),\n",
       "  array([0.030435], dtype=float32),\n",
       "  array([0.08005315], dtype=float32),\n",
       "  array([0.08005315], dtype=float32),\n",
       "  array([0.08005315], dtype=float32),\n",
       "  array([0.01261842], dtype=float32),\n",
       "  array([0.01261842], dtype=float32),\n",
       "  array([0.01261842], dtype=float32),\n",
       "  array([0.32522824], dtype=float32),\n",
       "  array([0.32522824], dtype=float32),\n",
       "  array([0.32522824], dtype=float32),\n",
       "  array([0.00299966], dtype=float32),\n",
       "  array([0.00299966], dtype=float32),\n",
       "  array([0.00299966], dtype=float32),\n",
       "  array([0.00306737], dtype=float32),\n",
       "  array([0.00306737], dtype=float32),\n",
       "  array([0.00306737], dtype=float32),\n",
       "  array([0.00258577], dtype=float32),\n",
       "  array([0.00258577], dtype=float32),\n",
       "  array([0.00258577], dtype=float32),\n",
       "  array([0.02960753], dtype=float32),\n",
       "  array([0.02960753], dtype=float32),\n",
       "  array([0.02960753], dtype=float32),\n",
       "  array([0.01614559], dtype=float32),\n",
       "  array([0.01614559], dtype=float32),\n",
       "  array([0.01614559], dtype=float32),\n",
       "  array([0.02630636], dtype=float32),\n",
       "  array([0.02630636], dtype=float32),\n",
       "  array([0.02630636], dtype=float32),\n",
       "  array([0.00503105], dtype=float32),\n",
       "  array([0.00503105], dtype=float32),\n",
       "  array([0.00503105], dtype=float32),\n",
       "  array([0.00309977], dtype=float32),\n",
       "  array([0.00309977], dtype=float32),\n",
       "  array([0.00309977], dtype=float32),\n",
       "  array([0.00463632], dtype=float32),\n",
       "  array([0.00463632], dtype=float32),\n",
       "  array([0.00463632], dtype=float32),\n",
       "  array([0.01474392], dtype=float32),\n",
       "  array([0.01474392], dtype=float32),\n",
       "  array([0.01474392], dtype=float32),\n",
       "  array([0.00215057], dtype=float32),\n",
       "  array([0.00215057], dtype=float32),\n",
       "  array([0.00215057], dtype=float32),\n",
       "  array([0.01482627], dtype=float32),\n",
       "  array([0.01482627], dtype=float32),\n",
       "  array([0.01482627], dtype=float32),\n",
       "  array([0.01582819], dtype=float32),\n",
       "  array([0.01582819], dtype=float32),\n",
       "  array([0.01582819], dtype=float32),\n",
       "  array([0.01475301], dtype=float32),\n",
       "  array([0.01475301], dtype=float32),\n",
       "  array([0.01475301], dtype=float32),\n",
       "  array([0.01516759], dtype=float32),\n",
       "  array([0.01516759], dtype=float32),\n",
       "  array([0.01516759], dtype=float32),\n",
       "  array([0.00292018], dtype=float32),\n",
       "  array([0.00292018], dtype=float32),\n",
       "  array([0.00292018], dtype=float32),\n",
       "  array([0.00275972], dtype=float32),\n",
       "  array([0.00275972], dtype=float32),\n",
       "  array([0.00275972], dtype=float32),\n",
       "  array([0.00914845], dtype=float32),\n",
       "  array([0.00914845], dtype=float32),\n",
       "  array([0.00914845], dtype=float32),\n",
       "  array([0.00171405], dtype=float32),\n",
       "  array([0.00171405], dtype=float32),\n",
       "  array([0.00171405], dtype=float32),\n",
       "  array([0.00177324], dtype=float32),\n",
       "  array([0.00177324], dtype=float32),\n",
       "  array([0.00177324], dtype=float32),\n",
       "  array([0.00137967], dtype=float32),\n",
       "  array([0.00137967], dtype=float32),\n",
       "  array([0.00137967], dtype=float32),\n",
       "  array([0.00385192], dtype=float32),\n",
       "  array([0.00385192], dtype=float32),\n",
       "  array([0.00385192], dtype=float32),\n",
       "  array([0.00352219], dtype=float32),\n",
       "  array([0.00352219], dtype=float32),\n",
       "  array([0.00352219], dtype=float32),\n",
       "  array([0.00476992], dtype=float32),\n",
       "  array([0.00476992], dtype=float32),\n",
       "  array([0.00476992], dtype=float32),\n",
       "  array([0.00219664], dtype=float32),\n",
       "  array([0.00219664], dtype=float32),\n",
       "  array([0.00219664], dtype=float32),\n",
       "  array([0.00260395], dtype=float32),\n",
       "  array([0.00260395], dtype=float32),\n",
       "  array([0.00260395], dtype=float32),\n",
       "  array([0.00129151], dtype=float32),\n",
       "  array([0.00129151], dtype=float32),\n",
       "  array([0.00129151], dtype=float32),\n",
       "  array([0.00354418], dtype=float32),\n",
       "  array([0.00354418], dtype=float32),\n",
       "  array([0.00354418], dtype=float32),\n",
       "  array([0.00459704], dtype=float32),\n",
       "  array([0.00459704], dtype=float32),\n",
       "  array([0.00459704], dtype=float32),\n",
       "  array([0.00249794], dtype=float32),\n",
       "  array([0.00249794], dtype=float32),\n",
       "  array([0.00249794], dtype=float32),\n",
       "  array([0.00235286], dtype=float32),\n",
       "  array([0.00235286], dtype=float32),\n",
       "  array([0.00235286], dtype=float32),\n",
       "  array([0.00347951], dtype=float32),\n",
       "  array([0.00347951], dtype=float32),\n",
       "  array([0.00347951], dtype=float32),\n",
       "  array([0.01463351], dtype=float32),\n",
       "  array([0.01463351], dtype=float32),\n",
       "  array([0.01463351], dtype=float32),\n",
       "  array([0.00483426], dtype=float32),\n",
       "  array([0.00483426], dtype=float32),\n",
       "  array([0.00483426], dtype=float32),\n",
       "  array([0.00387353], dtype=float32),\n",
       "  array([0.00387353], dtype=float32),\n",
       "  array([0.00387353], dtype=float32),\n",
       "  array([0.00279456], dtype=float32),\n",
       "  array([0.00279456], dtype=float32),\n",
       "  array([0.00279456], dtype=float32),\n",
       "  array([0.6939585], dtype=float32),\n",
       "  array([0.6939585], dtype=float32),\n",
       "  array([0.6939585], dtype=float32),\n",
       "  array([0.08757341], dtype=float32),\n",
       "  array([0.08757341], dtype=float32),\n",
       "  array([0.08757341], dtype=float32),\n",
       "  array([0.658506], dtype=float32),\n",
       "  array([0.658506], dtype=float32),\n",
       "  array([0.658506], dtype=float32),\n",
       "  array([0.00390601], dtype=float32),\n",
       "  array([0.00390601], dtype=float32),\n",
       "  array([0.00390601], dtype=float32),\n",
       "  array([0.00141507], dtype=float32),\n",
       "  array([0.00141507], dtype=float32),\n",
       "  array([0.00141507], dtype=float32),\n",
       "  array([0.01217026], dtype=float32),\n",
       "  array([0.01217026], dtype=float32),\n",
       "  array([0.01217026], dtype=float32),\n",
       "  array([0.00312808], dtype=float32),\n",
       "  array([0.00312808], dtype=float32),\n",
       "  array([0.00312808], dtype=float32),\n",
       "  array([0.00377786], dtype=float32),\n",
       "  array([0.00377786], dtype=float32),\n",
       "  array([0.00377786], dtype=float32),\n",
       "  array([0.00254416], dtype=float32),\n",
       "  array([0.00254416], dtype=float32),\n",
       "  array([0.00254416], dtype=float32),\n",
       "  array([0.00256613], dtype=float32),\n",
       "  array([0.00256613], dtype=float32),\n",
       "  array([0.00256613], dtype=float32),\n",
       "  array([0.00280216], dtype=float32),\n",
       "  array([0.00280216], dtype=float32),\n",
       "  array([0.00280216], dtype=float32),\n",
       "  array([0.00281075], dtype=float32),\n",
       "  array([0.00281075], dtype=float32),\n",
       "  array([0.00281075], dtype=float32),\n",
       "  array([0.00200474], dtype=float32),\n",
       "  array([0.00200474], dtype=float32),\n",
       "  array([0.00200474], dtype=float32),\n",
       "  array([0.00236654], dtype=float32),\n",
       "  array([0.00236654], dtype=float32),\n",
       "  array([0.00236654], dtype=float32),\n",
       "  array([0.00334471], dtype=float32),\n",
       "  array([0.00334471], dtype=float32),\n",
       "  array([0.00334471], dtype=float32),\n",
       "  array([0.74755514], dtype=float32),\n",
       "  array([0.74755514], dtype=float32),\n",
       "  array([0.74755514], dtype=float32),\n",
       "  array([0.2625956], dtype=float32),\n",
       "  array([0.2625956], dtype=float32),\n",
       "  array([0.2625956], dtype=float32),\n",
       "  array([0.7629018], dtype=float32),\n",
       "  array([0.7629018], dtype=float32),\n",
       "  array([0.7629018], dtype=float32),\n",
       "  array([0.00330785], dtype=float32),\n",
       "  array([0.00330785], dtype=float32),\n",
       "  array([0.00330785], dtype=float32),\n",
       "  array([0.00344619], dtype=float32),\n",
       "  array([0.00344619], dtype=float32),\n",
       "  array([0.00344619], dtype=float32),\n",
       "  array([0.002639], dtype=float32),\n",
       "  array([0.002639], dtype=float32),\n",
       "  array([0.002639], dtype=float32),\n",
       "  array([0.0013127], dtype=float32),\n",
       "  array([0.0013127], dtype=float32),\n",
       "  array([0.0013127], dtype=float32),\n",
       "  array([0.0014444], dtype=float32),\n",
       "  array([0.0014444], dtype=float32),\n",
       "  array([0.0014444], dtype=float32),\n",
       "  array([0.00227997], dtype=float32),\n",
       "  array([0.00227997], dtype=float32),\n",
       "  array([0.00227997], dtype=float32),\n",
       "  array([0.00342727], dtype=float32),\n",
       "  array([0.00342727], dtype=float32),\n",
       "  array([0.00342727], dtype=float32),\n",
       "  array([0.00294718], dtype=float32),\n",
       "  array([0.00294718], dtype=float32),\n",
       "  array([0.00294718], dtype=float32),\n",
       "  array([0.00264487], dtype=float32),\n",
       "  array([0.00264487], dtype=float32),\n",
       "  array([0.00264487], dtype=float32),\n",
       "  array([0.00618502], dtype=float32),\n",
       "  array([0.00618502], dtype=float32),\n",
       "  array([0.00618502], dtype=float32),\n",
       "  array([0.0065161], dtype=float32),\n",
       "  array([0.0065161], dtype=float32),\n",
       "  array([0.0065161], dtype=float32),\n",
       "  array([0.00826165], dtype=float32),\n",
       "  array([0.00826165], dtype=float32),\n",
       "  array([0.00826165], dtype=float32),\n",
       "  array([0.00666657], dtype=float32),\n",
       "  array([0.00666657], dtype=float32),\n",
       "  array([0.00666657], dtype=float32),\n",
       "  array([0.00525826], dtype=float32),\n",
       "  array([0.00525826], dtype=float32),\n",
       "  array([0.00525826], dtype=float32),\n",
       "  array([0.00306559], dtype=float32),\n",
       "  array([0.00306559], dtype=float32),\n",
       "  array([0.00306559], dtype=float32),\n",
       "  array([0.00797024], dtype=float32),\n",
       "  array([0.00797024], dtype=float32),\n",
       "  array([0.00797024], dtype=float32),\n",
       "  array([0.00377601], dtype=float32),\n",
       "  array([0.00377601], dtype=float32),\n",
       "  array([0.00377601], dtype=float32),\n",
       "  array([0.00408006], dtype=float32),\n",
       "  array([0.00408006], dtype=float32),\n",
       "  array([0.00408006], dtype=float32),\n",
       "  array([0.00306427], dtype=float32),\n",
       "  array([0.00306427], dtype=float32),\n",
       "  array([0.00306427], dtype=float32),\n",
       "  array([0.0053179], dtype=float32),\n",
       "  array([0.0053179], dtype=float32),\n",
       "  array([0.0053179], dtype=float32),\n",
       "  array([0.00770721], dtype=float32),\n",
       "  array([0.00770721], dtype=float32),\n",
       "  array([0.00770721], dtype=float32),\n",
       "  array([0.00416973], dtype=float32),\n",
       "  array([0.00416973], dtype=float32),\n",
       "  array([0.00416973], dtype=float32),\n",
       "  array([0.00649482], dtype=float32),\n",
       "  array([0.00649482], dtype=float32),\n",
       "  array([0.00649482], dtype=float32),\n",
       "  array([0.00222248], dtype=float32),\n",
       "  array([0.00222248], dtype=float32),\n",
       "  array([0.00222248], dtype=float32),\n",
       "  array([0.00139394], dtype=float32),\n",
       "  array([0.00139394], dtype=float32),\n",
       "  array([0.00139394], dtype=float32),\n",
       "  array([0.00129282], dtype=float32),\n",
       "  array([0.00129282], dtype=float32),\n",
       "  array([0.00129282], dtype=float32),\n",
       "  array([0.00171864], dtype=float32),\n",
       "  array([0.00171864], dtype=float32),\n",
       "  array([0.00171864], dtype=float32),\n",
       "  array([0.00677204], dtype=float32),\n",
       "  array([0.00677204], dtype=float32),\n",
       "  array([0.00677204], dtype=float32),\n",
       "  array([0.01092452], dtype=float32),\n",
       "  array([0.01092452], dtype=float32),\n",
       "  array([0.01092452], dtype=float32),\n",
       "  array([0.00573266], dtype=float32),\n",
       "  array([0.00573266], dtype=float32),\n",
       "  array([0.00573266], dtype=float32),\n",
       "  array([0.00358644], dtype=float32),\n",
       "  array([0.00358644], dtype=float32),\n",
       "  array([0.00358644], dtype=float32),\n",
       "  array([0.00510427], dtype=float32),\n",
       "  array([0.00510427], dtype=float32),\n",
       "  array([0.00510427], dtype=float32),\n",
       "  array([0.00264576], dtype=float32),\n",
       "  array([0.00264576], dtype=float32),\n",
       "  array([0.00264576], dtype=float32),\n",
       "  array([0.00220808], dtype=float32),\n",
       "  array([0.00220808], dtype=float32),\n",
       "  array([0.00220808], dtype=float32),\n",
       "  array([0.00218973], dtype=float32),\n",
       "  array([0.00218973], dtype=float32),\n",
       "  array([0.00218973], dtype=float32),\n",
       "  array([0.00138026], dtype=float32),\n",
       "  array([0.00138026], dtype=float32),\n",
       "  array([0.00138026], dtype=float32),\n",
       "  array([0.00453305], dtype=float32),\n",
       "  array([0.00453305], dtype=float32),\n",
       "  array([0.00453305], dtype=float32),\n",
       "  array([0.00617278], dtype=float32),\n",
       "  array([0.00617278], dtype=float32),\n",
       "  array([0.00617278], dtype=float32),\n",
       "  array([0.00421318], dtype=float32),\n",
       "  array([0.00421318], dtype=float32),\n",
       "  array([0.00421318], dtype=float32),\n",
       "  array([0.15374646], dtype=float32),\n",
       "  array([0.15374646], dtype=float32),\n",
       "  array([0.15374646], dtype=float32),\n",
       "  array([0.08101258], dtype=float32),\n",
       "  array([0.08101258], dtype=float32),\n",
       "  array([0.08101258], dtype=float32),\n",
       "  array([0.13247192], dtype=float32),\n",
       "  array([0.13247192], dtype=float32),\n",
       "  array([0.13247192], dtype=float32),\n",
       "  array([0.01568583], dtype=float32),\n",
       "  array([0.01568583], dtype=float32),\n",
       "  array([0.01568583], dtype=float32),\n",
       "  array([0.01394606], dtype=float32),\n",
       "  array([0.01394606], dtype=float32),\n",
       "  array([0.01394606], dtype=float32),\n",
       "  array([0.00961396], dtype=float32),\n",
       "  array([0.00961396], dtype=float32),\n",
       "  array([0.00961396], dtype=float32),\n",
       "  array([0.0735575], dtype=float32),\n",
       "  ...]]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "616.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(preds_test)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd_pred = []\n",
    "for p in pd_p:\n",
    "    if p == 1:\n",
    "        pd_pred.append(pd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_results = pd.concat([test_meta,pd.DataFrame(preds_test[0:20336])], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['signal_id', 'id_measurement', 'phase', 0], dtype='object')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>signal_id</th>\n",
       "      <th>id_measurement</th>\n",
       "      <th>phase</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>8820</td>\n",
       "      <td>2940</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>8821</td>\n",
       "      <td>2940</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>8822</td>\n",
       "      <td>2940</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>8823</td>\n",
       "      <td>2941</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>8824</td>\n",
       "      <td>2941</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>8825</td>\n",
       "      <td>2941</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>8826</td>\n",
       "      <td>2942</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>8827</td>\n",
       "      <td>2942</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>8828</td>\n",
       "      <td>2942</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>8838</td>\n",
       "      <td>2946</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>8839</td>\n",
       "      <td>2946</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>8840</td>\n",
       "      <td>2946</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>8844</td>\n",
       "      <td>2948</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>8845</td>\n",
       "      <td>2948</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>8846</td>\n",
       "      <td>2948</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>9201</td>\n",
       "      <td>3067</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>9202</td>\n",
       "      <td>3067</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>9203</td>\n",
       "      <td>3067</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>9285</td>\n",
       "      <td>3095</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>9286</td>\n",
       "      <td>3095</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>9287</td>\n",
       "      <td>3095</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>9381</td>\n",
       "      <td>3127</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>9382</td>\n",
       "      <td>3127</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>9383</td>\n",
       "      <td>3127</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>9522</td>\n",
       "      <td>3174</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>9523</td>\n",
       "      <td>3174</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>9524</td>\n",
       "      <td>3174</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>9528</td>\n",
       "      <td>3176</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>9529</td>\n",
       "      <td>3176</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>9530</td>\n",
       "      <td>3176</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19674</th>\n",
       "      <td>28386</td>\n",
       "      <td>9462</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19675</th>\n",
       "      <td>28387</td>\n",
       "      <td>9462</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19676</th>\n",
       "      <td>28388</td>\n",
       "      <td>9462</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19680</th>\n",
       "      <td>28392</td>\n",
       "      <td>9464</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19681</th>\n",
       "      <td>28393</td>\n",
       "      <td>9464</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19682</th>\n",
       "      <td>28394</td>\n",
       "      <td>9464</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20076</th>\n",
       "      <td>28788</td>\n",
       "      <td>9596</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20077</th>\n",
       "      <td>28789</td>\n",
       "      <td>9596</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20078</th>\n",
       "      <td>28790</td>\n",
       "      <td>9596</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20115</th>\n",
       "      <td>28827</td>\n",
       "      <td>9609</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20116</th>\n",
       "      <td>28828</td>\n",
       "      <td>9609</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20117</th>\n",
       "      <td>28829</td>\n",
       "      <td>9609</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20121</th>\n",
       "      <td>28833</td>\n",
       "      <td>9611</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20122</th>\n",
       "      <td>28834</td>\n",
       "      <td>9611</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20123</th>\n",
       "      <td>28835</td>\n",
       "      <td>9611</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20142</th>\n",
       "      <td>28854</td>\n",
       "      <td>9618</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20143</th>\n",
       "      <td>28855</td>\n",
       "      <td>9618</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20144</th>\n",
       "      <td>28856</td>\n",
       "      <td>9618</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20148</th>\n",
       "      <td>28860</td>\n",
       "      <td>9620</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20149</th>\n",
       "      <td>28861</td>\n",
       "      <td>9620</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20150</th>\n",
       "      <td>28862</td>\n",
       "      <td>9620</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20151</th>\n",
       "      <td>28863</td>\n",
       "      <td>9621</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20152</th>\n",
       "      <td>28864</td>\n",
       "      <td>9621</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20153</th>\n",
       "      <td>28865</td>\n",
       "      <td>9621</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20157</th>\n",
       "      <td>28869</td>\n",
       "      <td>9623</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20158</th>\n",
       "      <td>28870</td>\n",
       "      <td>9623</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20159</th>\n",
       "      <td>28871</td>\n",
       "      <td>9623</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20160</th>\n",
       "      <td>28872</td>\n",
       "      <td>9624</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20161</th>\n",
       "      <td>28873</td>\n",
       "      <td>9624</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20162</th>\n",
       "      <td>28874</td>\n",
       "      <td>9624</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>717 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       signal_id  id_measurement  phase    0\n",
       "108         8820            2940      0  1.0\n",
       "109         8821            2940      1  1.0\n",
       "110         8822            2940      2  1.0\n",
       "111         8823            2941      0  1.0\n",
       "112         8824            2941      1  1.0\n",
       "113         8825            2941      2  1.0\n",
       "114         8826            2942      0  1.0\n",
       "115         8827            2942      1  1.0\n",
       "116         8828            2942      2  1.0\n",
       "126         8838            2946      0  1.0\n",
       "127         8839            2946      1  1.0\n",
       "128         8840            2946      2  1.0\n",
       "132         8844            2948      0  1.0\n",
       "133         8845            2948      1  1.0\n",
       "134         8846            2948      2  1.0\n",
       "489         9201            3067      0  1.0\n",
       "490         9202            3067      1  1.0\n",
       "491         9203            3067      2  1.0\n",
       "573         9285            3095      0  1.0\n",
       "574         9286            3095      1  1.0\n",
       "575         9287            3095      2  1.0\n",
       "669         9381            3127      0  1.0\n",
       "670         9382            3127      1  1.0\n",
       "671         9383            3127      2  1.0\n",
       "810         9522            3174      0  1.0\n",
       "811         9523            3174      1  1.0\n",
       "812         9524            3174      2  1.0\n",
       "816         9528            3176      0  1.0\n",
       "817         9529            3176      1  1.0\n",
       "818         9530            3176      2  1.0\n",
       "...          ...             ...    ...  ...\n",
       "19674      28386            9462      0  1.0\n",
       "19675      28387            9462      1  1.0\n",
       "19676      28388            9462      2  1.0\n",
       "19680      28392            9464      0  1.0\n",
       "19681      28393            9464      1  1.0\n",
       "19682      28394            9464      2  1.0\n",
       "20076      28788            9596      0  1.0\n",
       "20077      28789            9596      1  1.0\n",
       "20078      28790            9596      2  1.0\n",
       "20115      28827            9609      0  1.0\n",
       "20116      28828            9609      1  1.0\n",
       "20117      28829            9609      2  1.0\n",
       "20121      28833            9611      0  1.0\n",
       "20122      28834            9611      1  1.0\n",
       "20123      28835            9611      2  1.0\n",
       "20142      28854            9618      0  1.0\n",
       "20143      28855            9618      1  1.0\n",
       "20144      28856            9618      2  1.0\n",
       "20148      28860            9620      0  1.0\n",
       "20149      28861            9620      1  1.0\n",
       "20150      28862            9620      2  1.0\n",
       "20151      28863            9621      0  1.0\n",
       "20152      28864            9621      1  1.0\n",
       "20153      28865            9621      2  1.0\n",
       "20157      28869            9623      0  1.0\n",
       "20158      28870            9623      1  1.0\n",
       "20159      28871            9623      2  1.0\n",
       "20160      28872            9624      0  1.0\n",
       "20161      28873            9624      1  1.0\n",
       "20162      28874            9624      2  1.0\n",
       "\n",
       "[717 rows x 4 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_results[first_results[0] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef, precision_recall_fscore_support"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
